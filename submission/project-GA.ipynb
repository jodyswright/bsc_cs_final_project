{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de2125be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files of size:\n",
      "Images: (14190, 281, 362, 1)\n",
      "Labels: (14190,)\n",
      "(11352, 281, 362, 1) (2838, 281, 362, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# pip install tensorflow Version: 2.17.0\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras import models, layers\n",
    "\n",
    "# Version: 3.4.1\n",
    "from tensorflow import keras\n",
    "\n",
    "# use for splitting the test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from scipy import spatial\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for the creation of the cartesian product grid\n",
    "from itertools import product\n",
    "\n",
    "# for the timing of each test\n",
    "import time\n",
    "\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "import pprint\n",
    "\n",
    "\n",
    "# the data tools module\n",
    "import data_tools as dt\n",
    "dh = dt.Data_Handling(output_size=5)\n",
    "\n",
    "# set the paths to load the processed data\n",
    "CURRENT_DIR = os.curdir\n",
    "label_path = f'{CURRENT_DIR}/data/mitdb_labels_reduced.npy'\n",
    "data_path = f'{CURRENT_DIR}/data/mitdb_data_reduced.npy'\n",
    "\n",
    "# get the data\n",
    "dh.load_data(label_path=label_path, data_path=data_path)\n",
    "\n",
    "# split into train and test sets\n",
    "dh.split_data(split=0.2)\n",
    "\n",
    "print(dh.X_train.shape, dh.X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd326062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Population:\n",
    "    '''\n",
    "    Class to create a population of genomes with each containing genes of random float values\n",
    "    \n",
    "    attributes:\n",
    "        layer_size\n",
    "        layer_shape\n",
    "    '''\n",
    "    def __init__(self, max_depth=5):\n",
    "        \n",
    "        self.__max_depth = max_depth\n",
    "\n",
    "        self.__global_spec = {\n",
    "            'learning_rate':[0.1, 0.01, 0.001, 0.0001],\n",
    "            'optimizer':['adam', 'sgd'],\n",
    "            'start_size':list(range(8, 33, 8)),\n",
    "            'batch_size':list(range(64, 257, 32))\n",
    "        }\n",
    "        self.__layer_spec = {\n",
    "            'filter_size':[i/10 for i in range(11, 20)],\n",
    "            'filter_activation': ['relu'],\n",
    "            'dropout_exists': [True, False],\n",
    "            'dropout_rate': [i/10 for i in range(2, 6, 1)],\n",
    "            'max_pool_exists': [True, False],\n",
    "            'max_pool_size' : [2, 3]\n",
    "        }\n",
    "    \n",
    "    def get_genome(self, depth=1):\n",
    "        '''\n",
    "        Creates a genome to include a global gene and at least 1 layer\n",
    "        \n",
    "        Params:\n",
    "            depth (int) the number of layers within the genome\n",
    "\n",
    "        Returns:\n",
    "            (np.array) a set of genes\n",
    "        '''\n",
    "        assert depth >= 1, \"Size must be at least 1\"\n",
    "\n",
    "        # global gene\n",
    "        genome = []\n",
    "        genome.append(self.get_gene(len(self.__global_spec)))\n",
    "        # layer genes\n",
    "        [genome.append(self.get_gene(len(self.__layer_spec))) for g in range(depth)]\n",
    "        \n",
    "        return genome\n",
    "    \n",
    "    \n",
    "    def get_gene(self, size=4):\n",
    "        '''\n",
    "        Creates gene of the given size with a set number of decimal places\n",
    "        \n",
    "        Params:\n",
    "            size (int) the number of values within the gene\n",
    "        \n",
    "        Returns:\n",
    "            (np.array) a single gene\n",
    "        '''\n",
    "        \n",
    "#         create a random gene of the given size\n",
    "        gene = list(np.random.rand(size))\n",
    "\n",
    "        return list(gene)\n",
    "    \n",
    "    def get_population(self, size=3):\n",
    "        '''\n",
    "        Creates a population of the given size with random length genomes\n",
    "        \n",
    "        Params:\n",
    "            pop_size (int) the number random genomes to return\n",
    "        \n",
    "        Returns:\n",
    "            (np.array) a set of genomes\n",
    "        '''\n",
    "        \n",
    "        population = [\n",
    "            self.get_genome(\n",
    "            np.random.randint(1, self.__max_depth + 1)) for i in range(size)\n",
    "        ]\n",
    "\n",
    "        return population\n",
    "    \n",
    "    def map_gene(self, gene, spec):\n",
    "        \n",
    "        mapped_gene = {}\n",
    "        \n",
    "        for i, (k, v) in enumerate(spec.items()):\n",
    "\n",
    "            map_idx = int(math.floor(gene[i] * len(v)))\n",
    "            mapped_gene[k] = v[map_idx]\n",
    "        \n",
    "        return mapped_gene\n",
    "            \n",
    "    def map_genome(self, genome):\n",
    "        \n",
    "        mapped_genomes = {}\n",
    "        mapped_genomes['global_params'] = self.map_gene(genome[0], self.__global_spec)\n",
    "        \n",
    "        mapped_genomes['layer_params'] = {\n",
    "            i: self.map_gene(genome[i], self.__layer_spec) \n",
    "         for i in range(1, len(genome))\n",
    "        }\n",
    "        \n",
    "        return mapped_genomes\n",
    "    \n",
    "    def map_population(self, population):\n",
    "        \n",
    "        mapped_population = [self.map_genome(g) for g in population]\n",
    "        \n",
    "        return mapped_population "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07a3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Constructor:\n",
    "    '''\n",
    "    handles the stages of mapping a genome to the required set of \n",
    "    hyper-parameter values of various types. these are used to build and\n",
    "    compile a training ready model\n",
    "\n",
    "    parameters\n",
    "        shape (tuple) specifies the input shape of the data to be modelled eg (28, 28, 1) \n",
    "        output_size (int) the number of classes to be modelled eg 5\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, shape, output_size):\n",
    "        \n",
    "        self.__shape = shape\n",
    "        self.__output_size = output_size\n",
    "        \n",
    "    def build_model(self, parameters):\n",
    "        '''\n",
    "        builds a keras model of varying depth.\n",
    "        the depth is defined by the length of the layer_params\n",
    "        layers such as dropout and pooling are added if the parameters specifies true\n",
    "        each layer will have at least 1 conv2d and at most conv2d, maxpool, dropout\n",
    "        parameters are defined by the two sets passed in\n",
    "        \n",
    "        parameters:\n",
    "            parameters (dict) set of mapped parameters\n",
    "            \n",
    "        returns:\n",
    "            (keras.model) a compliled model ready for training\n",
    "        \n",
    "        '''\n",
    "        global_params = parameters['global_params']\n",
    "        layer_params = parameters['layer_params']\n",
    "        # used for the scaling of each filter size\n",
    "        prev_size = global_params['start_size']\n",
    "        # new empty model\n",
    "        model = keras.Sequential()\n",
    "    #     # set the input shape\n",
    "        model.add(keras.Input(shape=self.__shape))\n",
    "        \n",
    "        for i, key in enumerate(layer_params.keys()):\n",
    "            # extract the current layer's parameters to make code more readable\n",
    "            params = layer_params[key]\n",
    "            # calculate the layer size base on the previous size\n",
    "            layer_size = int(params['filter_size'] * prev_size)\n",
    "            \n",
    "            # CONV2D\n",
    "            model.add(\n",
    "                keras.layers.Conv2D(\n",
    "                    filters=layer_size, \n",
    "                    kernel_size=3, \n",
    "                    activation=params['filter_activation']\n",
    "                )\n",
    "            )\n",
    "            # MAX POOL\n",
    "            model.add(\n",
    "                keras.layers.MaxPooling2D(\n",
    "                    pool_size=params['max_pool_size']\n",
    "                )\n",
    "            )\n",
    "            # DROPOUT\n",
    "            if params['dropout_exists']:\n",
    "                # only add a droput layer if param = true\n",
    "                model.add(\n",
    "                    keras.layers.Dropout(\n",
    "                        rate=params['dropout_rate']\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            prev_size = layer_size\n",
    "            \n",
    "        # OUTPUT\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(self.__output_size, activation=\"softmax\"))      \n",
    "\n",
    "        # compile the model\n",
    "        model.compile(optimizer=global_params['optimizer'],\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5038411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dict, name):\n",
    "    \n",
    "    filename = CURRENT_DIR\n",
    "    filename += '/ga_results/'\n",
    "    filename += f'{name}.json'\n",
    "\n",
    "    f = open(filename, \"w\")\n",
    "\n",
    "    json.dump(dict, f, indent = 6)\n",
    "\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "917b44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evolution:\n",
    "    '''\n",
    "    Requires the Genome class using the static functions - instance not required\n",
    "    \n",
    "    Params:\n",
    "            probability (float) the rate of probability eg 1 is 100% and 0.5 is 50%\n",
    "            highest_is_fittest (bool) if true then higher values are considered fitter (eg accuracy)\n",
    "                          if false thenlower values are considered fitter (eg loss)\n",
    "            aggression (float) a higher number gives more weight to the fitter values giving a higher\n",
    "                                probability of these being chosen in the selection function\n",
    "            global_len (int) the length of the global gene in which each paramter represented by a single value\n",
    "            mutation_amount (float [0.0, 0.1]) the amount of mutation to be applied to any gene value \n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        probability=0.1,\n",
    "        highest_is_fittest=True, \n",
    "        aggression=2,\n",
    "        global_len=4,  \n",
    "        mutation_amount=0.1,\n",
    "        max_depth=5\n",
    "    ):\n",
    "        self.__probability = probability\n",
    "        self.__highest_is_fittest = highest_is_fittest \n",
    "        self.__aggression = aggression\n",
    "        self.__global_len = global_len \n",
    "        self.__mutation_amount = mutation_amount\n",
    "        self.__max_depth = max_depth\n",
    "\n",
    "    def probability_test(self):\n",
    "        '''\n",
    "        Returns true if the random number is below that of the rate parameter\n",
    "        This gives a probabilty test for any defined operations\n",
    "        \n",
    "        Returns:\n",
    "            (bool) true if the random number is less than the rate parameter\n",
    "        '''\n",
    "        return True if np.random.rand() < self.__probability else False\n",
    "\n",
    "    def select_fittest(self, fits):\n",
    "        '''\n",
    "        selects the fittest value from a list using the roulette method and returns \n",
    "        the index value of the original source list which can then be applied to a \n",
    "        list of genes or parameters if build using the same order\n",
    "        \n",
    "        parameters:\n",
    "            fits (array float) fitness values for a set of models\n",
    "\n",
    "        returns:\n",
    "            (int) the index value of the fittest value in the list\n",
    "        '''\n",
    "        # arbitary figure to ensure all significant values are integers\n",
    "        int_scale = 10000\n",
    "        \n",
    "        if self.__highest_is_fittest:\n",
    "            fits = (np.array(fits) * int_scale).astype(int)\n",
    "        else:\n",
    "#           invert the values in order to prioritize the lowest\n",
    "            fits = (1 /np.array(fits) * int_scale).astype(int)\n",
    "        # order the values such that the larger have a wider interval\n",
    "        cum_array = (np.cumsum(np.sort(fits)) * self.__aggression).astype(int)\n",
    "        # choose random int between zero and max of the cum array scaled by the exponent 1/aggression\n",
    "        random_idx = int((np.random.rand() ** (1/self.__aggression)) * cum_array[-1])\n",
    "#         find the corresponding index value within the cum array\n",
    "        idx = np.searchsorted(cum_array, random_idx, side=\"left\")\n",
    "\n",
    "        # Get the sorted indices of the array\n",
    "        sorted_indices = np.argsort(fits)\n",
    "    #     extrapolate back the index to that of the corresponding value in the fits array\n",
    "        res = sorted_indices[idx]\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def point_mutate(self, genome):\n",
    "        '''\n",
    "        Randomly mutates a gene or genes within the given set of genes. \n",
    "        The gene values are either increased or decreased depending on the random number -0.5 to 0.5. \n",
    "\n",
    "        Params:\n",
    "            genome (np.array) the set of genes to be mutated\n",
    "\n",
    "        Returns:\n",
    "            (list) the mutated set of genes\n",
    "        '''\n",
    "\n",
    "        for gene in genome:\n",
    "            for i, value in enumerate(gene):\n",
    "                gene[i] = self.mutate_gene_value(value)\n",
    "            \n",
    "        return genome\n",
    "    \n",
    "    def mutate_gene_value(self, gene_value):\n",
    "        \n",
    "        if self.probability_test():\n",
    "            \n",
    "            mutation = (np.random.rand() - 0.5) * self.__mutation_amount\n",
    "            gene_value = gene_value + mutation\n",
    "            # fix value in the interval [0.0, 0.99] \n",
    "            # to ensure the index remains within the spec values\n",
    "            gene_value = min(max(gene_value, 0), 0.99)\n",
    "            \n",
    "        return gene_value\n",
    "    \n",
    "    def shrink_mutate(self, genome):\n",
    "        '''\n",
    "        Probabilistically removes one gene to the genes set resulting in one less layer\n",
    "        \n",
    "        Params:\n",
    "            genome (np.array) set of genes to be mutated\n",
    "            \n",
    "        Returns:\n",
    "            (np.array) set of genes with len genes or genes - 1\n",
    "        '''\n",
    "        min_depth = 1\n",
    "#         ensure that the deep layer count is always >= 1\n",
    "\n",
    "        depth = self.get_genome_depth(genome)\n",
    "\n",
    "        if depth <= min_depth: return genome\n",
    "\n",
    "        if self.probability_test():\n",
    "            idx = np.random.randint(1, depth)\n",
    "            genome = genome[:-idx]\n",
    "\n",
    "        return genome\n",
    "    \n",
    "    def grow_mutate(self, genome):\n",
    "        '''\n",
    "        Probabilistically adds one gene to the genes set, resulting in another layer\n",
    "        \n",
    "        Params:\n",
    "            genome (np.array) set of genes to be mutated\n",
    "            \n",
    "        Returns:\n",
    "            (np.array) set of genes with len genes or genes + 1\n",
    "        '''\n",
    "        new_genome = []\n",
    "        layer_width = len(genome[-1])\n",
    "        depth = self.get_genome_depth(genome)\n",
    "        \n",
    "#       ensure layers never exceeds the maximum number of layers\n",
    "#       split at the global params index to get layers len\n",
    "        if depth >= self.__max_depth:\n",
    "               return genome\n",
    "\n",
    "        if self.probability_test():\n",
    "        # adds a new layer but not exceeding the max layers\n",
    "            new_gene = pn.get_gene(size=layer_width)\n",
    "            genome = genome + list([new_gene])\n",
    "                \n",
    "        return genome\n",
    "    \n",
    "    def crossover(self, genome_1, genome_2):\n",
    "        '''\n",
    "        Merges two genes at a random point to create a new \"child\" gene\n",
    "        \n",
    "        Params:\n",
    "            gene1 (np.array) single gene\n",
    "            gene2 (np.array) single gene\n",
    "            \n",
    "        Returns:\n",
    "            (np.array) set of genes with len genes or genes + 1\n",
    "        '''\n",
    "\n",
    "#         simplify the genomes such that the global can be divided\n",
    "        global_len = len(genome_1[0])\n",
    "        # parent 1\n",
    "        p1 = genome_1[0].copy()\n",
    "        p1.extend(genome_1[1:])\n",
    "        # parent 2\n",
    "        p2 = genome_2[0].copy()\n",
    "        p2.extend(genome_2[1:])\n",
    "#         use min() to ensure the index remains in the bounds \n",
    "#         of the smaller gene array and min_gene ensures at least one gene is crossed\n",
    "        min_gene = 1\n",
    "        min_len = len(p1)\n",
    "        split_idx = random.randint(1, random.randint(min_gene, min_len))\n",
    "        merge = p1[:split_idx] + p2[split_idx:]\n",
    "        # reshape into global and layer genes\n",
    "        child = [merge[:global_len]] + merge[global_len:]\n",
    "    \n",
    "        return child\n",
    "    \n",
    "    def get_genome_depth(self, genome):\n",
    "#         return the number of layers minus the global gene\n",
    "        return len(genome) - 1\n",
    "    \n",
    "    def get_next_generation(self, fits, pop):\n",
    "        '''\n",
    "        create a population of the size defined in the class parameters\n",
    "        \n",
    "        parameters:\n",
    "            fits (array) set of floats taken from each models evaluation performance\n",
    "            pop (2d array) the current population under evaluation\n",
    "        '''\n",
    "\n",
    "        next_generation = []\n",
    "        for i in range(len(fits)):\n",
    "\n",
    "#             find the index two fit parents\n",
    "            p1 = self.select_fittest(fits)\n",
    "            p2 = self.select_fittest(fits)\n",
    "\n",
    "#             get the genome of each parent\n",
    "            gn1 = pop[p1].copy()\n",
    "            gn2 = pop[p2].copy()\n",
    "            \n",
    "#             mate the parents\n",
    "            child = self.crossover(gn1, gn2)\n",
    "#             print(child)\n",
    "    #             apply the point mutation to the child\n",
    "            child = self.point_mutate(child)\n",
    "    #             grow, shrink or retain the depth using the probability applied within the functions\n",
    "            size_mutate = {\n",
    "                1: self.grow_mutate(child),\n",
    "                2: self.shrink_mutate(child),\n",
    "                3: child}\n",
    "            p = np.random.randint(1, 4)\n",
    "            child = size_mutate[p]\n",
    "\n",
    "            next_generation.append(child.copy())\n",
    "            \n",
    "        return next_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6e837df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_population(mapped_pop, X, y, max_epochs, validation_split=0.2, callbacks=[], monitor_string=\"\"):\n",
    "    \n",
    "    best_results = []\n",
    "    best_epochs = []\n",
    "    best_so_far = float(\"inf\")\n",
    "\n",
    "    for i, genome in enumerate(mapped_pop):\n",
    "        # map the gene and build the model\n",
    "        model = mc.build_model(genome)\n",
    "        g = genome['global_params']\n",
    "        l = genome['layer_params']\n",
    "        # monitor metrics \n",
    "        depth = len(l.keys())\n",
    "        batch_size = g['batch_size']\n",
    "        \n",
    "        # process update to user\n",
    "        clear_output()\n",
    "        print(f'{monitor_string}')\n",
    "        print(f'Genome   \\t| {i + 1} of {len(mapped_pop)}')\n",
    "        print(f'Depth \\t\\t| {depth}')\n",
    "        print(f'Batch size \\t| {batch_size}\\n')\n",
    "        print(f'Best this gen  {best_so_far}\\n')\n",
    "        \n",
    "        # fit to the data \n",
    "        model.fit( \n",
    "                    X, \n",
    "                    y,\n",
    "                    epochs=max_epochs, \n",
    "                    validation_split=validation_split, \n",
    "                    batch_size=g['batch_size'], \n",
    "                    callbacks=[callbacks],\n",
    "                    verbose=1\n",
    "                )\n",
    "        \n",
    "        # get the training history\n",
    "        hist = model.history.history['loss']\n",
    "    #   store best fitness and the best epoch for survival of the fittest\n",
    "        best_result = min(hist)\n",
    "        # for monitoring during the test\n",
    "        best_so_far = min(best_so_far, best_result)\n",
    "        # for output for use in the Evolution class\n",
    "        best_epochs.append(len(hist))\n",
    "        best_results.append(best_result)\n",
    "        \n",
    "        \n",
    "    return best_results, best_epochs\n",
    "    \n",
    "\n",
    "def evolve_the_model(\n",
    "    X, y, generations, pop_size, start_epochs, epoch_factor, fitness_func, test_name):\n",
    "    \n",
    "    # create the callbacks\n",
    "    monitor = 'loss'\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(\n",
    "            monitor=monitor, \n",
    "            patience=3,\n",
    "            mode='auto'\n",
    "    )]\n",
    "    results = {}\n",
    "    file_path = 'ga_results.json'\n",
    "    \n",
    "    epochs=start_epochs\n",
    "    # for monitoring\n",
    "    best_so_far = float(\"inf\")\n",
    "    # get initial population of size n\n",
    "    pop = pn.get_population(size=pop_size)\n",
    "    mapped_pop = pn.map_population(pop)\n",
    "    \n",
    "    results = {\n",
    "        'Best_parameters': [],\n",
    "        'Best_fitness': [],\n",
    "        'Fitness_values': [],\n",
    "        'Best_so_far': [],\n",
    "        'Best_epochs' : []\n",
    "    }\n",
    "    # evolve for the number of generations\n",
    "    for generation in range(generations):\n",
    "        \n",
    "        monitor_string = f'Generation\\t| {generation + 1} of {generations}\\n'\n",
    "        monitor_string += f'Max epochs \\t| {epochs}\\n'\n",
    "        monitor_string += f'Best so far\\t| {best_so_far}\\n'\n",
    "        \n",
    "        fits, best_epochs = test_population(\n",
    "            mapped_pop=mapped_pop, \n",
    "            X=X, \n",
    "            y=y, \n",
    "            max_epochs=epochs, \n",
    "            validation_split=0.2, \n",
    "            callbacks=callbacks, \n",
    "            monitor_string=monitor_string)\n",
    "        \n",
    "        \n",
    "        fittest = min(fits)\n",
    "        best_so_far = min(best_so_far, fittest)\n",
    "        best_genome = pop[np.argmin(fits)]\n",
    "        best_params = mapped_pop[np.argmin(fits)]\n",
    "\n",
    "        results['Best_parameters'].append(best_params)\n",
    "        results['Best_fitness'].append(fittest) \n",
    "        results['Fitness_values'].append(fits)\n",
    "        results['Best_epochs'].append(best_epochs)\n",
    "        results['Best_so_far'].append(best_so_far)\n",
    "        \n",
    "        # save each run in case of a crash\n",
    "        save_dict(results, test_name)\n",
    "        # increase the number of epochs incrementally\n",
    "        epochs = epochs + epoch_factor\n",
    "    # get next pop from the Evolution class of size n-1\n",
    "    # add back the fittest of the previous generation\n",
    "        pop = fitness_func(fits, pop)[:-1]\n",
    "        pop.append(best_genome)\n",
    "        mapped_pop = pn.map_population(pop)\n",
    "    return results\n",
    "    # can also reduce the population using the same idea - try as an experiment\n",
    "    \n",
    "    # also try using the early stopping as a bespoke in that if performance has\n",
    "    # not improved for n generations then end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29fd4b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation\t| 1 of 10\n",
      "Max epochs \t| 2\n",
      "Best so far\t| inf\n",
      "\n",
      "Genome   \t| 1 of 10\n",
      "Depth \t\t| 3\n",
      "Batch size \t| 192\n",
      "\n",
      "Best this gen  inf\n",
      "\n",
      "Epoch 1/2\n",
      "\u001b[1m 1/48\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:34\u001b[0m 5s/step - accuracy: 0.0625 - loss: 66.2898"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m dh\u001b[38;5;241m.\u001b[39mX_train\n\u001b[1;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m dh\u001b[38;5;241m.\u001b[39my_train\n\u001b[0;32m---> 16\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevolve_the_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfitness_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_generation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_name\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 84\u001b[0m, in \u001b[0;36mevolve_the_model\u001b[0;34m(X, y, generations, pop_size, start_epochs, epoch_factor, fitness_func, test_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m monitor_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMax epochs \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m| \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     82\u001b[0m monitor_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest so far\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m| \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_so_far\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 84\u001b[0m fits, best_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_population\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapped_pop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapped_pop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m fittest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(fits)\n\u001b[1;32m     95\u001b[0m best_so_far \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(best_so_far, fittest)\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mtest_population\u001b[0;34m(mapped_pop, X, y, max_epochs, validation_split, callbacks, monitor_string)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest this gen  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_so_far\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# fit to the data \u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# get the training history\u001b[39;00m\n\u001b[1;32m     36\u001b[0m hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/ml3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# population to create the inital population and mappings\n",
    "pn = Population(max_depth=4)   \n",
    "# new model constructor to handle the build and training of each genome\n",
    "mc = Model_Constructor(shape=dh.shape, output_size=5)\n",
    "# new evloution class to handle the genes\n",
    "ev = Evolution(probability=0.5, highest_is_fittest=False, mutation_amount=0.1, max_depth=4)\n",
    "\n",
    "# for the puprose of testing set the seed to the usual answer to life the universe and everything\n",
    "keras.utils.set_random_seed(42)\n",
    "# use the time so as to not inadvertantly overwrite previous tests\n",
    "test_name = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "X = dh.X_train\n",
    "y = dh.y_train\n",
    "\n",
    "results = evolve_the_model(\n",
    "    X=X, y=y, \n",
    "    generations=10, \n",
    "    pop_size=10, \n",
    "    start_epochs=2, \n",
    "    epoch_factor=1, \n",
    "    fitness_func=ev.get_next_generation,\n",
    "    test_name=test_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild and train on entire set\n",
    "model = mc.build_model(g, l)\n",
    "vaidation_split = 0\n",
    "\n",
    "mc.train_model(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    epochs=max_epochs, \n",
    "    split=vaidation_split, \n",
    "    batch=g['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GA_Optimizer:\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.__X = X\n",
    "        self.__y = y\n",
    "        \n",
    "    def test_population(self):\n",
    "        pass\n",
    "    \n",
    "    def evolve_the_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596086a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[1] = {'population':list(pop[0][4])}\n",
    "save_dict(results)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e90048",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pn.get_population(pop_size=5)\n",
    "\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d531884",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results, best_epochs = test_population(\n",
    "    population=pop, X=X, y=y, max_epochs=2, validation_split=0.2, callbacks=[], monitor_string=\"Test\"\n",
    ")\n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d209af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see twitter bookmark\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "with alive_bar() as bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300deff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X, y, batch_size=g['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee16a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53847dd8",
   "metadata": {},
   "source": [
    "### Repeatability\n",
    "- The default initializer is random_glorot with a default seed=None.   \n",
    "- This, according to keras, produces a deterministic set of values https://keras.io/api/layers/initializers/  \n",
    "- \"Note that an initializer seeded with an integer or None (unseeded) will produce the same random values across multiple calls\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e5941",
   "metadata": {},
   "source": [
    "### For the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd17892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pop = pn.get_population(pop_size=5)\n",
    "genome = pop[0]\n",
    "pprint.pp(genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f72eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_params, layer_params = mc.get_param_dict(genome)\n",
    "pprint.pp(global_params)\n",
    "pprint.pp(layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d776224",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mc.build_model(global_params, layer_params)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = [0.5,0.3,1,0.6,0.1]\n",
    "next_gen = ev.get_next_generation(fits, pop)\n",
    "print(f'Pop size: {[len(g) for g in pop]}')\n",
    "print(f'Next gen: {[len(g) for g in next_gen]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e53601",
   "metadata": {},
   "outputs": [],
   "source": [
    "child = ev.crossover(pop[0].copy(), pop[1].copy())\n",
    "child\n",
    "# ev.point_mutate(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec653815",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e516ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "child = []\n",
    "c1 = pop[0][:idx].copy()\n",
    "c2 = pop[1][idx:].copy()\n",
    "print(f'{c1}\\n{c2}')\n",
    "\n",
    "child.extend(c1)\n",
    "child.extend(c2)\n",
    "print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# fits = [30, 20,10,10000,500,600,700,0.1]\n",
    "# fits = [0.4,0.3,0.6,0.1,0.7,0.2,0.18,0.1]\n",
    "fits = [1,2,3,4,5]\n",
    "# fits = best_results\n",
    "results = []\n",
    "pop = pn.get_population(pop_size=20)\n",
    "for i in range(1000):\n",
    "    res = ev.select_fittest(fits)\n",
    "    results.append(fits[res])\n",
    "    \n",
    "unique, counts = np.unique(results, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ec93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = ev.select_fittest(best_results)\n",
    "print(best_results, best_results[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop = pn.get_population(pop_size=5)\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b192d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pop_test = pn.get_population(pop_size=10)\n",
    "c1 = pop_test[0]\n",
    "c2 = pop_test[1]\n",
    "print(f'C1{c1}\\n\\nC2{c2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698cf847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c0, idx = ev.crossover(c1, c2)\n",
    "print(f'idx: {idx}\\n\\nC0{c0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mutated_genome = ev.grow_mutate(c1)\n",
    "mutated_genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutated_genome = ev.shrink_mutate(c1)\n",
    "mutated_genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = c1.copy()\n",
    "mutated_genome = ev.point_mutate(genome)\n",
    "mutated_genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32faec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use in the model builder to assign a default in the case of a missing parameter\n",
    "d = {'test':100}\n",
    "d.get('t', 'default')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c65bfb",
   "metadata": {},
   "source": [
    "# Redundant Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant\n",
    "# class Model_Handler:\n",
    "    \n",
    "#     def __init__(self, shape, output_size, global_size, layer_size):\n",
    "#         self.__shape = shape\n",
    "#         self.__output_size = output_size\n",
    "#         # the size of the global parameters eg global_params = params[:global_size]\n",
    "#         self.__global_size = global_size\n",
    "#         # the size of each layer's parameters eg n arrays of width layer_size\n",
    "#         self.__layer_size = layer_size\n",
    "\n",
    "# #     def build_model(self, global_params, layer_params):\n",
    "        \n",
    "# #         # used for the scaling of each filter size\n",
    "# #         prev_size = global_params[2]\n",
    "# #         # new empty model\n",
    "# #         model = keras.Sequential()\n",
    "# #         # set the input shape\n",
    "# #         model.add(keras.Input(shape=self.__shape))\n",
    "# #         # using a reshaped layer parameter array loop each and apply\n",
    "# #         for size_scale, dropout, rate in layer_params:\n",
    "# #             layer_size = int(size_scale * prev_size)\n",
    "# #             # set the layer size as a multiple of the previous using the size_scale param\n",
    "# #             model.add(keras.layers.Conv2D(filters=layer_size, kernel_size=3, activation=\"relu\"))\n",
    "# #             model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
    "# #             # only add a droput layer if param = true\n",
    "# #             if dropout:\n",
    "# #                 model.add(keras.layers.Dropout(rate=rate))\n",
    "\n",
    "# #             prev_size = layer_size\n",
    "\n",
    "# #         # add the final layers of the model\n",
    "# #         model.add(keras.layers.Flatten())\n",
    "# #         model.add(keras.layers.Dense(self.__output_size, activation=\"softmax\"))      \n",
    "\n",
    "# #         # compile the model\n",
    "# #         model.compile(optimizer=\"rmsprop\",\n",
    "# #                       loss=\"sparse_categorical_crossentropy\",\n",
    "# #                       metrics=[\"accuracy\"])\n",
    "\n",
    "# #         return model\n",
    "    \n",
    "#     def train_model(model, X, y, epochs, split, batch, callbacks):\n",
    "#         model.fit(\n",
    "#         dh.X_train[:1000], \n",
    "#         dh.y_train[:1000], \n",
    "#         epochs=max_epochs, \n",
    "#         validation_split=val_split, \n",
    "#         batch_size=batch_size, \n",
    "#         callbacks=[callbacks]\n",
    "#     )\n",
    "        \n",
    "#     def test_population(self, population):\n",
    "#         # split into global and layer params\n",
    "#         global_params = params[:self.__global_size]\n",
    "#         layer_params = np.array(params[self.__global_size:]).reshape(self.__layer_shape)\n",
    "        \n",
    "#         for genome in population:\n",
    "#             print(genome)\n",
    "# #             model = self.build_model(global_params, layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant\n",
    "# mh = Model_Handler(\n",
    "#     shape=(281, 362, 1), \n",
    "#     output_size=5, \n",
    "#     global_size=population.global_size, \n",
    "#     layer_shape=population.layer_shape\n",
    "# )\n",
    "# # model = mh.build_model(global_params, layer_params)\n",
    "# # model.summary()\n",
    "\n",
    "# mh.test_population(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ad159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant\n",
    "# # set the params\n",
    "# max_epochs = 5\n",
    "# vaidation_split = 0.2\n",
    "# batch_size = 256\n",
    "\n",
    "# # create the callbacks\n",
    "# monitor = 'val_loss'\n",
    "# checkpoint_path = 'checkpoint_path.keras'\n",
    "\n",
    "# callbacks = [\n",
    "#     keras.callbacks.EarlyStopping(\n",
    "#         monitor=monitor, \n",
    "#         patience=3\n",
    "#     ),\n",
    "    \n",
    "# #     keras.callbacks.ModelCheckpoint(\n",
    "# #         filepath=checkpoint_path, \n",
    "# #         monitor=monitor, \n",
    "# #         save_best_only=True\n",
    "# #     )\n",
    "# ]\n",
    "\n",
    "# model.fit(\n",
    "#     dh.X_train[:1000], \n",
    "#     dh.y_train[:1000], \n",
    "#     epochs=max_epochs, \n",
    "#     validation_split=vaidation_split, \n",
    "#     batch_size=batch_size, \n",
    "#     callbacks=[callbacks]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant\n",
    "# initializer = keras.initializers.Ones()\n",
    "# layer = keras.layers.Conv2D(filters=layer_size, kernel_size=3, activation=\"relu\", kernel_initializer=initializer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3",
   "language": "python",
   "name": "ml3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
