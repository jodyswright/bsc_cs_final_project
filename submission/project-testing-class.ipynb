{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f94d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mail\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Env using Python 3.10.14\n",
    "# pip install notebook==5.7.5\n",
    "\n",
    "# the keras libraries\n",
    "# pip install tensorflow Version: 2.17.0\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras import models, layers\n",
    "\n",
    "# Version: 3.4.1\n",
    "from tensorflow import keras\n",
    "\n",
    "# pip install keras-tuner\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23edb2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# other libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use for splitting the test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for the creation of the cartesian product grid\n",
    "from itertools import product\n",
    "\n",
    "# for the timing of each test\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa786f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991e404",
   "metadata": {},
   "source": [
    "# Basic conv net to achieve statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f6f07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Handling:\n",
    "    '''\n",
    "    requirements:\n",
    "        math\n",
    "        numpy\n",
    "        train_test_split from sklearn.model_selection\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, output_size=5, batch_size=128, ):\n",
    "        self.__shape = 0\n",
    "        self.__output_size = output_size\n",
    "        self.__length = 0\n",
    "        \n",
    "        \n",
    "    def load_data(self, label_path, data_path, split=0.2):\n",
    "        # load the images and labels\n",
    "        self.__labels = np.load(label_path)\n",
    "        self.__data = np.load(data_path)\n",
    "        self.__length = self.__labels.shape[0]\n",
    "        self.__shape = self.__data[1].shape\n",
    "        \n",
    "\n",
    "        print(\"Loaded files of size:\")\n",
    "        print(f\"Images: {self.__data.shape}\\nLabels: {self.__labels.shape}\")\n",
    "        \n",
    "    def split_data(self, split=0.2):\n",
    "        # split and shuffle the data and labels\n",
    "        self.__X_train, self.__X_test, self.__y_train, self.__y_test = train_test_split(\n",
    "            self.__data, self.__labels, test_size=split)\n",
    "        \n",
    "    \n",
    "    def run_tuner(timer, tuner, max_epochs=50, batch_size=64, callbacks=[]):\n",
    "        SPLIT = 0.2\n",
    "        timer.start(test_name)\n",
    "\n",
    "        tuner.search(\n",
    "            self.__X_train, self.__y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=max_epochs,\n",
    "            validation_split=SPLIT,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        # get and view the best performing hyper parameter set\n",
    "        best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "        test_duration = timer.stop(best_hps.values)\n",
    "        best_hps.values\n",
    "\n",
    "    @property\n",
    "    def shape(self, ):\n",
    "        return self.__shape\n",
    "    \n",
    "    @property\n",
    "    def output_size(self, ):\n",
    "        return self.__output_size\n",
    "    \n",
    "    @property\n",
    "    def X_train(self, ):\n",
    "        return self.__X_train\n",
    "    \n",
    "    @property\n",
    "    def y_train(self, ):\n",
    "        return self.__y_train\n",
    "    \n",
    "    @property\n",
    "    def X_test(self, ):\n",
    "        return self.__X_test\n",
    "    \n",
    "    @property\n",
    "    def y_test(self, ):\n",
    "        return self.__y_test\n",
    "    \n",
    "    @property\n",
    "    def length(self, ):\n",
    "        return self.__length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39055730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files of size:\n",
      "Images: (14190, 281, 362, 1)\n",
      "Labels: (14190,)\n"
     ]
    }
   ],
   "source": [
    "dh = Data_Handling()\n",
    "\n",
    "label_path = 'mitdb_labels_reduced.npy'\n",
    "data_path = 'mitdb_data_reduced.npy'\n",
    "\n",
    "dh.load_data(label_path=label_path, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872d62db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11352, 281, 362, 1) (2838, 281, 362, 1)\n"
     ]
    }
   ],
   "source": [
    "dh.split_data(split=0.2)\n",
    "print(dh.X_train.shape, dh.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e6dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# must prefix keras. unlike in the book\n",
    "def build_basic_model(shape, output_size, callbacks):\n",
    "    \n",
    "    inputs = keras.Input(shape=shape)   \n",
    "    x = keras.layers.Conv2D(filters=2, kernel_size=3, activation=\"relu\")(inputs) \n",
    "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    \n",
    "    outputs = keras.layers.Dense(output_size, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975d1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the params\n",
    "max_epochs = 50\n",
    "vaidation_split = 0.2\n",
    "batch_size = 256\n",
    "\n",
    "# create the callbacks\n",
    "monitor = 'val_loss'\n",
    "checkpoint_path = 'checkpoint_path.keras'\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=monitor, \n",
    "        patience=3\n",
    "    ),\n",
    "    \n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        monitor=monitor, \n",
    "        save_best_only=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build fresh model\n",
    "basic_model = build_basic_model(dh.shape, dh.output_size, callbacks)\n",
    "\n",
    "# fit to the taining data\n",
    "basic_model.fit(\n",
    "    dh.X_train, \n",
    "    dh.y_train, \n",
    "    epochs=max_epochs, \n",
    "    validation_split=vaidation_split, \n",
    "    batch_size=batch_size, \n",
    "    callbacks=[callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb153e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the epoch as per the early stopping callback above\n",
    "val_loss_history = basic_model.history.history['val_loss']\n",
    "# get the index of the lowest recorded loss function (+ 1 to account for 0 idx)\n",
    "best_epoch = np.argmin(val_loss_history) + 1\n",
    "\n",
    "\n",
    "# build and train a fresh model for evaluation\n",
    "basic_test_model = build_basic_model(dh.shape, dh.output_size, callbacks)\n",
    "# fit model on the entire training set by removing the validation_split param\n",
    "basic_model_history = basic_test_model.fit(\n",
    "    dh.X_train, \n",
    "    dh.y_train, \n",
    "    epochs=best_epoch, \n",
    "    batch_size=batch_size, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance on the unseen test data to see whether the basic model can beat \n",
    "# a the statistical significance calculated in the workbook [WORKBOOK]\n",
    "basic_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = basic_model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.argmax(pred, axis = 1)[:5] \n",
    "r = y_test[:5]\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55041e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ed097",
   "metadata": {},
   "source": [
    "## Grid search using keras library functinon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e82924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperModel(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self, num_classes, shape, filter_step=16, lr_step=0.1 ):\n",
    "\n",
    "            self.__num_classes = num_classes\n",
    "            self.__shape = shape\n",
    "            self.__filter_step = filter_step\n",
    "            self.__lr_step = lr_step\n",
    "            \n",
    "    def build(self, hp):\n",
    "\n",
    "        filters_1 = hp.Int(name=\"filters_1\", min_value=16, max_value=32, step=self.__filter_step) \n",
    "        filters_2 = hp.Int(name=\"filters_2\", min_value=filters_1, max_value=64, step=self.__filter_step)\n",
    "        # allows a zero setting \n",
    "        rate_1 = hp.Float(name=\"rate_1\", min_value=0, max_value=0.5, step=self.__lr_step)\n",
    "        \n",
    "        inputs = keras.Input(shape=self.__shape)\n",
    "        x = keras.layers.Conv2D(filters=filters_1, kernel_size=3, activation=\"relu\")(inputs) \n",
    "        x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = keras.layers.Dropout(rate=rate_1)(x)\n",
    "        x = keras.layers.Conv2D(filters=filters_2, kernel_size=3, activation=\"relu\")(x) \n",
    "        x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "        x = keras.layers.Flatten()(x)\n",
    "        \n",
    "        outputs = keras.layers.Dense(self.__num_classes, activation=\"softmax\")(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0718148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner_Timer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__start = None\n",
    "        self.__end = None\n",
    "        self.__duration = None\n",
    "#         self.__results = {}\n",
    "#         self.__tuners = []\n",
    "#         self.__common_params = common_params\n",
    "#         self.__callbacks = []\n",
    "        \n",
    "    def add_tuner(self, tuner):\n",
    "        self.__tuners.append(tuner)\n",
    "        \n",
    "    def start(self):\n",
    "        '''\n",
    "        start the timer \n",
    "        '''\n",
    "        self.__start = time.time()\n",
    "        \n",
    "    def stop(self):\n",
    "        '''\n",
    "        stop the timer and format the duration\n",
    "        '''\n",
    "        self.__end = time.time()\n",
    "        self.__duration = self.__end - self.__start\n",
    "        duration_string = time.strftime('%H:%M:%S', time.gmtime(self.__duration))\n",
    "\n",
    "        return duration_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3dcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_result(results_dict, results, tuner):\n",
    "\n",
    "#     if tuner.project_name in results_dict.keys():\n",
    "        \n",
    "#         results_dict[tuner.project_name].update(results)\n",
    "# #         print(results)\n",
    "#     else:\n",
    "#         results_dict[tuner.project_name] = results\n",
    "        \n",
    "    #save the current result to file\n",
    "    f = open(f\"{tuner.directory}/results.json\", \"w\")\n",
    "\n",
    "    json.dump(results, f, indent = 6)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded3b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tuner(timer, tuner, batch_size, max_epochs, callbacks):\n",
    "        '''\n",
    "        run a given tuner saving the best parameter configuration to a timer object\n",
    "        along with the total duration of the optimizer's run\n",
    "\n",
    "        params:\n",
    "            timer (Tuner_Timer) the timer used for recording results\n",
    "            tuner (keras.Tuner) the current tuner under test \n",
    "            batch_size (int) batch for training \n",
    "            max_epochs (int) the maximum number of epochs to run if not stopped by early stopping\n",
    "            callbacks (keras.callbacks) for early stopping\n",
    "        '''\n",
    "        # start timer\n",
    "        print(f'Running optimizer {tuner.project_name}')\n",
    "        timer.start()\n",
    "        # the tuner will save results to the directory specified in the tuner constructor\n",
    "        tuner.search(\n",
    "            dh.X_train, dh.y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=max_epochs,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        # get the best performing hyper parameter set\n",
    "        best_hps = tuner.get_best_hyperparameters()[0].values\n",
    "        # stop timing and get the duration\n",
    "        test_duration = timer.stop()\n",
    "        \n",
    "        return {'duration':test_duration, 'best_params':best_hps}\n",
    "#         return {tuner.tuner_id:{'duration':'00:00:10', 'best_params':{'1':100, '2':200}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a381526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_and_evaluate(tuner):\n",
    "    # get the object hp to rebuild a fresh model\n",
    "    best_hps = tuners[0].get_best_hyperparameters()[0]\n",
    "    # build a fresh model for retraining in order to find the point of overfitting\n",
    "    model = hypermodel.build(best_hps)\n",
    "    print('Finding best epoch')\n",
    "    model.fit(dh.X_train[:1000], dh.y_train[:1000],\n",
    "                batch_size=batch_size,\n",
    "                epochs=max_epochs,\n",
    "                validation_split=0.2,\n",
    "                callbacks=callbacks,)\n",
    "\n",
    "    # find best epoch since there seems no way to find this in the tuner\n",
    "    best_epoch = np.argmin(model.history.history['val_loss'])\n",
    "    # rebuild fresh model\n",
    "    print(f'\\nRetraining to best epoch: {best_epoch}')\n",
    "    model = hypermodel.build(best_hps)\n",
    "    # reterain on the entire set for the best epoch\n",
    "    model.fit(dh.X_train, dh.y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=best_epoch,)\n",
    "    \n",
    "    print('\\nEvaluating model')\n",
    "    loss, accuracy = model.evaluate(dh.X_test[:1000], dh.y_test[:1000])\n",
    "    return loss, accuracy, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50947227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start-08-22\n"
     ]
    }
   ],
   "source": [
    "hypermodel = HyperModel(num_classes=dh.output_size, shape=dh.shape, filter_step=16)\n",
    "now = str(datetime.datetime.now())[5:10]\n",
    "\n",
    "directory = f\"start-{now}\"\n",
    "print(directory)\n",
    "\n",
    "common_params = {\n",
    "    'hypermodel': hypermodel, \n",
    "    'objective': \"val_accuracy\", \n",
    "    'executions_per_trial':1,\n",
    "    'directory':directory,\n",
    "    'tuner_id':'test_03',\n",
    "    'overwrite':False,\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2fc846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 30\n",
    "\n",
    "timer = Tuner_Timer()\n",
    "best_results = {}\n",
    "tuners = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e57f1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM\n",
    "tuners.append(kt.RandomSearch(project_name='random', max_trials=40, **common_params))\n",
    "\n",
    "# HYPERBAND\n",
    "tuners.append(kt.Hyperband(project_name='hyperband', factor=3, hyperband_iterations=1, **common_params))\n",
    "\n",
    "# BAYES\n",
    "tuners.append(kt.BayesianOptimization(project_name='bayes', **common_params))\n",
    "\n",
    "# GRID\n",
    "tuners.append(kt.GridSearch(project_name='grid', **common_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55979b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 Complete [00h 06m 59s]\n",
      "val_accuracy: 0.5741963982582092\n",
      "\n",
      "Best val_accuracy So Far: 0.9625715613365173\n",
      "Total elapsed time: 08h 32m 17s\n",
      "\n",
      "Search: Running Trial #27\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "16                |32                |filters_1\n",
      "32                |32                |filters_2\n",
      "0                 |0                 |rate_1\n",
      "0.0001            |0.0001            |learning_rate\n",
      "\n",
      "Epoch 1/30\n",
      "71/71 [==============================] - 69s 965ms/step - loss: 208.1658 - accuracy: 0.5252 - val_loss: 10.6759 - val_accuracy: 0.8653\n",
      "Epoch 2/30\n",
      "71/71 [==============================] - 67s 939ms/step - loss: 19.1177 - accuracy: 0.8152 - val_loss: 7.8735 - val_accuracy: 0.8613\n",
      "Epoch 3/30\n",
      "71/71 [==============================] - 65s 916ms/step - loss: 5.6759 - accuracy: 0.9086 - val_loss: 2.2797 - val_accuracy: 0.9467\n",
      "Epoch 4/30\n",
      "71/71 [==============================] - 66s 931ms/step - loss: 1.3994 - accuracy: 0.9458 - val_loss: 1.1389 - val_accuracy: 0.9397\n",
      "Epoch 5/30\n",
      "71/71 [==============================] - 65s 914ms/step - loss: 0.3135 - accuracy: 0.9724 - val_loss: 0.4388 - val_accuracy: 0.9639\n",
      "Epoch 6/30\n",
      "23/71 [========>.....................] - ETA: 41s - loss: 0.0383 - accuracy: 0.9901"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run all optimizers\n",
    "for tuner in tuners:\n",
    "    \n",
    "    # run the optimizer\n",
    "    best_result = run_tuner(timer, tuner, batch_size, max_epochs, callbacks)\n",
    "    \n",
    "#     # add best params to result dict and save current dict to file\n",
    "#     save_best_result(best_results, best_result, tuner)\n",
    "    if tuner.project_name in best_results.keys():\n",
    "        \n",
    "        best_results[tuner.project_name].update(best_result)\n",
    "#         print(results)\n",
    "    else:\n",
    "        best_results[tuner.project_name] = best_result\n",
    "    \n",
    "    loss, accuracy, best_epoch = retrain_and_evaluate(tuners[0])\n",
    "    \n",
    "    best_results[tuner.project_name]['loss'] = loss\n",
    "    best_results[tuner.project_name]['accuracy'] = accuracy\n",
    "    #convert from int64 to make serializable\n",
    "    best_results[tuner.project_name]['best_epoch'] = int(best_epoch)\n",
    "    \n",
    "     #save the current result to file\n",
    "    f = open(f\"{tuner.directory}/results.json\", \"w\")\n",
    "\n",
    "    json.dump(best_results, f, indent = 6)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    print(f'Loss: {loss}\\tAccuracy: {accuracy}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d28829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde39636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuners[0].results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0736a595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-08-19'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "str(now)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31e378",
   "metadata": {},
   "source": [
    "[cite]  \n",
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + logfactor(max_epochs) and rounding it up to the nearest integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af7c5a",
   "metadata": {},
   "source": [
    "## Redundant code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "     0:\n",
    "      {'hyperband':\n",
    "       {'r1':100,'r2':150},\n",
    "      'bayes':\n",
    "       {'r1':100,'r2':150}\n",
    "      },\n",
    "      1:\n",
    "      {'hyperband':\n",
    "       {'r1':100,'r2':150},\n",
    "      'bayes':\n",
    "       {'r1':100,'r2':150}\n",
    "      }\n",
    "     }\n",
    "    \n",
    "d[2] = {\"hyperband\":{}}\n",
    "d[2]['hyperband'] = {'r1':999,'r2':999}\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f07064",
   "metadata": {},
   "source": [
    "### Results\n",
    "Error \"BiasGrad requires tensor size <= int32 max\" with batch 256  \n",
    "https://stackoverflow.com/questions/60414562/how-to-solve-the-biasgrad-requires-tensor-size-int32-max-invalidargumenterr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622febbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
