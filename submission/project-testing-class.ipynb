{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f94d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mail\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Env using Python 3.10.14\n",
    "# pip install notebook==5.7.5\n",
    "\n",
    "# the keras libraries\n",
    "# pip install tensorflow Version: 2.17.0\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras import models, layers\n",
    "\n",
    "# Version: 3.4.1\n",
    "from tensorflow import keras\n",
    "\n",
    "# pip install keras-tuner\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23edb2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# other libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use for splitting the test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for the creation of the cartesian product grid\n",
    "from itertools import product\n",
    "\n",
    "# for the timing of each test\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa786f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991e404",
   "metadata": {},
   "source": [
    "# Basic conv net to achieve statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f6f07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Handling:\n",
    "    '''\n",
    "    requirements:\n",
    "        math\n",
    "        numpy\n",
    "        train_test_split from sklearn.model_selection\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, output_size=5, batch_size=128, ):\n",
    "        self.__shape = 0\n",
    "        self.__output_size = output_size\n",
    "        self.__length = 0\n",
    "        \n",
    "        \n",
    "    def load_data(self, label_path, data_path, split=0.2):\n",
    "        # load the images and labels\n",
    "        self.__labels = np.load(label_path)\n",
    "        self.__data = np.load(data_path)\n",
    "        self.__length = self.__labels.shape[0]\n",
    "        self.__shape = self.__data[1].shape\n",
    "        \n",
    "\n",
    "        print(\"Loaded files of size:\")\n",
    "        print(f\"Images: {self.__data.shape}\\nLabels: {self.__labels.shape}\")\n",
    "        \n",
    "    def split_data(self, split=0.2):\n",
    "        # split and shuffle the data and labels\n",
    "        self.__X_train, self.__X_test, self.__y_train, self.__y_test = train_test_split(\n",
    "            self.__data, self.__labels, test_size=split)\n",
    "        \n",
    "    \n",
    "#     def run_tuner(timer, tuner, max_epochs=50, batch_size=64, callbacks=[]):\n",
    "#         SPLIT = 0.2\n",
    "#         timer.start(test_name)\n",
    "\n",
    "#         tuner.search(\n",
    "#             self.__X_train, self.__y_train,\n",
    "#             batch_size=batch_size,\n",
    "#             epochs=max_epochs,\n",
    "#             validation_split=SPLIT,\n",
    "#             callbacks=callbacks,\n",
    "#         )\n",
    "#         # get and view the best performing hyper parameter set\n",
    "#         best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "#         test_duration = timer.stop(best_hps.values)\n",
    "#         best_hps.values\n",
    "\n",
    "    @property\n",
    "    def shape(self, ):\n",
    "        return self.__shape\n",
    "    \n",
    "    @property\n",
    "    def output_size(self, ):\n",
    "        return self.__output_size\n",
    "    \n",
    "    @property\n",
    "    def X_train(self, ):\n",
    "        return self.__X_train\n",
    "    \n",
    "    @property\n",
    "    def y_train(self, ):\n",
    "        return self.__y_train\n",
    "    \n",
    "    @property\n",
    "    def X_test(self, ):\n",
    "        return self.__X_test\n",
    "    \n",
    "    @property\n",
    "    def y_test(self, ):\n",
    "        return self.__y_test\n",
    "    \n",
    "    @property\n",
    "    def length(self, ):\n",
    "        return self.__length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39055730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files of size:\n",
      "Images: (14190, 281, 362, 1)\n",
      "Labels: (14190,)\n"
     ]
    }
   ],
   "source": [
    "dh = Data_Handling()\n",
    "\n",
    "label_path = '../data/mitdb_labels_reduced.npy'\n",
    "data_path = '../data/mitdb_data_reduced.npy'\n",
    "\n",
    "dh.load_data(label_path=label_path, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872d62db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11352, 281, 362, 1) (2838, 281, 362, 1)\n"
     ]
    }
   ],
   "source": [
    "dh.split_data(split=0.2)\n",
    "print(dh.X_train.shape, dh.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0e6dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# must prefix keras. unlike in the book\n",
    "def build_basic_model(shape, output_size, callbacks):\n",
    "    \n",
    "    inputs = keras.Input(shape=shape)   \n",
    "    x = keras.layers.Conv2D(filters=2, kernel_size=3, activation=\"relu\")(inputs) \n",
    "    x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    \n",
    "    outputs = keras.layers.Dense(output_size, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ae20b29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mail\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mail\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mail\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the basic model and save the weights for use thorughout the testing \n",
    "model = build_basic_model(dh.shape, dh.output_size, callbacks=[])\n",
    "model.save_weights('../weights/my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d7e3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the params\n",
    "max_epochs = 50\n",
    "vaidation_split = 0.2\n",
    "batch_size = 256\n",
    "\n",
    "# create the callbacks\n",
    "metric = 'val_loss'\n",
    "checkpoint_path = 'checkpoint_path.keras'\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=metric, \n",
    "        patience=3\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "32ec6a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "36/36 [==============================] - 16s 431ms/step - loss: 5469.6602 - accuracy: 0.3279 - val_loss: 3228.5994 - val_accuracy: 0.4963\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 14s 402ms/step - loss: 1304.4900 - accuracy: 0.5211 - val_loss: 250.0755 - val_accuracy: 0.7741\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 14s 388ms/step - loss: 304.1337 - accuracy: 0.7556 - val_loss: 71.6806 - val_accuracy: 0.8657\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 14s 401ms/step - loss: 99.4130 - accuracy: 0.8649 - val_loss: 53.4566 - val_accuracy: 0.8670\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 14s 399ms/step - loss: 23.3137 - accuracy: 0.9288 - val_loss: 19.5362 - val_accuracy: 0.9199\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 14s 392ms/step - loss: 6.2674 - accuracy: 0.9597 - val_loss: 9.5197 - val_accuracy: 0.9410\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 15s 419ms/step - loss: 1.8780 - accuracy: 0.9760 - val_loss: 7.5498 - val_accuracy: 0.9185\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 15s 419ms/step - loss: 0.6207 - accuracy: 0.9866 - val_loss: 5.9301 - val_accuracy: 0.9410\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 15s 417ms/step - loss: 0.3543 - accuracy: 0.9907 - val_loss: 6.4717 - val_accuracy: 0.9309\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 15s 421ms/step - loss: 0.2251 - accuracy: 0.9930 - val_loss: 5.1536 - val_accuracy: 0.9454\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 15s 421ms/step - loss: 0.0779 - accuracy: 0.9958 - val_loss: 4.8848 - val_accuracy: 0.9445\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 14s 401ms/step - loss: 0.0780 - accuracy: 0.9965 - val_loss: 5.2421 - val_accuracy: 0.9467\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 14s 391ms/step - loss: 0.0541 - accuracy: 0.9971 - val_loss: 5.0978 - val_accuracy: 0.9463\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 15s 407ms/step - loss: 0.0354 - accuracy: 0.9976 - val_loss: 4.8872 - val_accuracy: 0.9476\n"
     ]
    }
   ],
   "source": [
    "# set seed to zero to be the same as that of the optimiser tests\n",
    "# seed = 42\n",
    "# tf.keras.utils.set_random_seed(seed) \n",
    "# build fresh model\n",
    "basic_model = build_basic_model(dh.shape, dh.output_size, callbacks)\n",
    "basic_model.load_weights('../weights/my_model_weights.h5')\n",
    "\n",
    "# fit to the taining data\n",
    "history = basic_model.fit(\n",
    "    dh.X_train, \n",
    "    dh.y_train, \n",
    "    epochs=max_epochs, \n",
    "    validation_split=vaidation_split, \n",
    "    batch_size=batch_size, \n",
    "    callbacks=[callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fb153e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "45/45 [==============================] - 18s 381ms/step - loss: 4825.4570 - accuracy: 0.3307\n",
      "Epoch 2/11\n",
      "45/45 [==============================] - 17s 380ms/step - loss: 747.3987 - accuracy: 0.6426\n",
      "Epoch 3/11\n",
      "45/45 [==============================] - 17s 386ms/step - loss: 137.8913 - accuracy: 0.8294\n",
      "Epoch 4/11\n",
      "45/45 [==============================] - 17s 383ms/step - loss: 28.7769 - accuracy: 0.9227\n",
      "Epoch 5/11\n",
      "45/45 [==============================] - 17s 383ms/step - loss: 6.3042 - accuracy: 0.9583\n",
      "Epoch 6/11\n",
      "45/45 [==============================] - 16s 353ms/step - loss: 2.3356 - accuracy: 0.9691\n",
      "Epoch 7/11\n",
      "45/45 [==============================] - 17s 372ms/step - loss: 0.6828 - accuracy: 0.9845\n",
      "Epoch 8/11\n",
      "45/45 [==============================] - 17s 385ms/step - loss: 0.2793 - accuracy: 0.9897\n",
      "Epoch 9/11\n",
      "45/45 [==============================] - 17s 371ms/step - loss: 0.2441 - accuracy: 0.9928\n",
      "Epoch 10/11\n",
      "45/45 [==============================] - 17s 387ms/step - loss: 0.1077 - accuracy: 0.9946\n",
      "Epoch 11/11\n",
      "45/45 [==============================] - 17s 387ms/step - loss: 0.1000 - accuracy: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d999f2820>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the epoch as per the early stopping callback above\n",
    "train_history = history.history[metric]\n",
    "# get the index of the lowest recorded loss function (+ 1 to account for 0 idx)\n",
    "best_epoch = get_best_epoch[metric](train_history) + 1\n",
    "\n",
    "# build and train a fresh model for evaluation\n",
    "basic_model = build_basic_model(dh.shape, dh.output_size, callbacks=[])\n",
    "basic_model.load_weights('../weights/my_model_weights.h5')\n",
    "# fit model on the entire training set by removing the validation_split param\n",
    "basic_model.fit(\n",
    "    dh.X_train, \n",
    "    dh.y_train, \n",
    "    epochs=best_epoch, \n",
    "    batch_size=batch_size, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a72a0d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 2s 92ms/step - loss: 4.3531 - accuracy: 0.9475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.353099822998047, 0.9474982619285583]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate performance on the unseen test data to see whether the basic model can beat \n",
    "# a the statistical significance calculated in the workbook [WORKBOOK]\n",
    "basic_model.evaluate(dh.X_test, dh.y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef1f180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 2s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = basic_model.predict(dh.X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06ba3480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_epoch = {\n",
    "    'accuracy': lambda x: np.argmax(x),\n",
    "    'val_accuracy' : lambda x: np.argmax(x),\n",
    "    'loss': lambda x: np.argmin(x),\n",
    "    'val_loss' : lambda x: np.argmin(x) \n",
    "}\n",
    "\n",
    "metric = 'val_accuracy'\n",
    "get_best_epoch[metric](history.history[metric])   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a25a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 1 ... 4 1 2] [4 1 1 ... 4 1 2]\n"
     ]
    }
   ],
   "source": [
    "p = np.argmax(pred, axis = 1)\n",
    "r =dh.y_test\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9392901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 2s 17ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m p \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax (p)\n\u001b[0;32m      3\u001b[0m y_test\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(dh\u001b[38;5;241m.\u001b[39my_test)\n\u001b[1;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m(y_test, y_prediction , normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "p = model.predict(dh.X_test)\n",
    "p = np.argmax (p)\n",
    "y_test=np.argmax(dh.y_test)\n",
    "\n",
    "result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55041e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ed097",
   "metadata": {},
   "source": [
    "## Grid search using keras library functinon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8e82924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperModel(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self, num_classes, shape, filter_step=16, lr_step=0.1, weights=None):\n",
    "\n",
    "            self.__num_classes = num_classes\n",
    "            self.__shape = shape\n",
    "            self.__filter_step = filter_step\n",
    "            self.__lr_step = lr_step\n",
    "            self.__weights = weights\n",
    "            \n",
    "    def build(self, hp):\n",
    "\n",
    "        filters_1 = hp.Int(name=\"filters_1\", min_value=16, max_value=32, step=self.__filter_step) \n",
    "        filters_2 = hp.Int(name=\"filters_2\", min_value=32, max_value=64, step=self.__filter_step)\n",
    "        # allows a zero setting \n",
    "        rate_1 = hp.Float(name=\"rate_1\", min_value=0.2, max_value=0.5, step=self.__lr_step)\n",
    "        \n",
    "        inputs = keras.Input(shape=self.__shape)\n",
    "        x = keras.layers.Conv2D(filters=filters_1, kernel_size=3, activation=\"relu\")(inputs) \n",
    "        x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = keras.layers.Dropout(rate=rate_1)(x)\n",
    "        x = keras.layers.Conv2D(filters=filters_2, kernel_size=3, activation=\"relu\")(x) \n",
    "        x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "        x = keras.layers.Flatten()(x)\n",
    "        \n",
    "        outputs = keras.layers.Dense(self.__num_classes, activation=\"softmax\")(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "        \n",
    "        # to ensure a level playing field between optimisers\n",
    "        if self.__weights:\n",
    "            model.load_weights('../weights/opt_test_weights.h5')\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0669b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a set of initialized weights for the model\n",
    "hp1 = kt.HyperParameters()\n",
    "filter_step=8\n",
    "\n",
    "hp1.Int(name=\"filters_1\", min_value=16, max_value=32, step=filter_step) \n",
    "hp1.Int(name=\"filters_2\", min_value=16, max_value=64, step=filter_step)\n",
    "rate_1 = hp1.Float(name=\"rate_1\", min_value=0, max_value=0.5, step=0.1)\n",
    "hp1.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "hpm = HyperModel(num_classes=dh.output_size, shape=dh.shape, filter_step=16)\n",
    "model = hpm.build(hp1)\n",
    "weights_path = '../weights/opt_test_weights.h5'\n",
    "model.save_weights(weights_path)\n",
    "\n",
    "model.save('../models/untrained.keras')\n",
    "# del model\n",
    "# load_model('../models/untrained.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0718148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tuner_Timer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__start = None\n",
    "        self.__end = None\n",
    "        self.__duration = None\n",
    "#         self.__results = {}\n",
    "#         self.__tuners = []\n",
    "#         self.__common_params = common_params\n",
    "#         self.__callbacks = []\n",
    "        \n",
    "    def add_tuner(self, tuner):\n",
    "        self.__tuners.append(tuner)\n",
    "        \n",
    "    def start(self):\n",
    "        '''\n",
    "        start the timer \n",
    "        '''\n",
    "        self.__start = time.time()\n",
    "        \n",
    "    def stop(self):\n",
    "        '''\n",
    "        stop the timer and format the duration\n",
    "        '''\n",
    "        self.__end = time.time()\n",
    "        self.__duration = self.__end - self.__start\n",
    "        duration_string = time.strftime('%H:%M:%S', time.gmtime(self.__duration))\n",
    "\n",
    "        return duration_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9c3dcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_result(results_dict, results, tuner):\n",
    "\n",
    "#     if tuner.project_name in results_dict.keys():\n",
    "        \n",
    "#         results_dict[tuner.project_name].update(results)\n",
    "# #         print(results)\n",
    "#     else:\n",
    "#         results_dict[tuner.project_name] = results\n",
    "        \n",
    "    #save the current result to file\n",
    "    f = open(f\"{tuner.directory}/results.json\", \"w\")\n",
    "\n",
    "    json.dump(results, f, indent = 6)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ded3b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tuner(timer, tuner, batch_size, max_epochs, callbacks):\n",
    "        '''\n",
    "        run a given tuner saving the best parameter configuration to a timer object\n",
    "        along with the total duration of the optimizer's run\n",
    "\n",
    "        params:\n",
    "            timer (Tuner_Timer) the timer used for recording results\n",
    "            tuner (keras.Tuner) the current tuner under test \n",
    "            batch_size (int) batch for training \n",
    "            max_epochs (int) the maximum number of epochs to run if not stopped by early stopping\n",
    "            callbacks (keras.callbacks) for early stopping\n",
    "        '''\n",
    "        # start timer\n",
    "        print(f'Running optimizer {tuner.project_name}')\n",
    "        timer.start()\n",
    "        # the tuner will save results to the directory specified in the tuner constructor\n",
    "        tuner.search(\n",
    "            dh.X_train, dh.y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=max_epochs,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # get the best performing hyper parameter set\n",
    "        best_hps = tuner.get_best_hyperparameters()[0].values\n",
    "        # stop timing and get the duration\n",
    "        test_duration = timer.stop()\n",
    "        \n",
    "        return {'duration':test_duration, 'best_params':best_hps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a381526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_and_evaluate(tuner):\n",
    "    # get the object hp to rebuild a fresh model\n",
    "    best_hps = tuners[0].get_best_hyperparameters()[0]\n",
    "    # build a fresh model for retraining in order to find the point of overfitting\n",
    "    model = hypermodel.build(best_hps)\n",
    "    print('Finding best epoch')\n",
    "    model.fit(dh.X_train[:1000], dh.y_train[:1000],\n",
    "                batch_size=batch_size,\n",
    "                epochs=max_epochs,\n",
    "                validation_split=0.2,\n",
    "                callbacks=callbacks,)\n",
    "\n",
    "    # find best epoch since there seems no way to find this in the tuner\n",
    "    best_epoch = np.argmin(model.history.history['val_accuracy'])\n",
    "    # rebuild fresh model\n",
    "    print(f'\\nRetraining to best epoch: {best_epoch}')\n",
    "    model = hypermodel.build(best_hps)\n",
    "    # reterain on the entire set for the best epoch\n",
    "    model.fit(dh.X_train, dh.y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=best_epoch,)\n",
    "    \n",
    "    print('\\nEvaluating model')\n",
    "    loss, accuracy = model.evaluate(dh.X_test, dh.y_test)\n",
    "    return loss, accuracy, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "50947227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start-08-28\n"
     ]
    }
   ],
   "source": [
    "hypermodel = HyperModel(num_classes=dh.output_size, shape=dh.shape, filter_step=16)\n",
    "now = str(datetime.datetime.now())[5:10]\n",
    "\n",
    "directory = f\"start-{now}\"\n",
    "print(directory)\n",
    "metrics = \"val_loss\"\n",
    "\n",
    "common_params = {\n",
    "#     'hypermodel': hypermodel, \n",
    "    'objective': metrics, \n",
    "    'executions_per_trial':1,\n",
    "    'directory':directory,\n",
    "    'tuner_id':'test_03',\n",
    "    'overwrite':False,\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=metrics, patience=3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f2fc846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 30\n",
    "\n",
    "timer = Tuner_Timer()\n",
    "best_results = {}\n",
    "tuners = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e57f1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM\n",
    "tuners.append(kt.RandomSearch(build_optimizer_model, project_name='random', max_trials=40, **common_params))\n",
    "\n",
    "# HYPERBAND\n",
    "tuners.append(kt.Hyperband(build_optimizer_model, project_name='hyperband', factor=3, hyperband_iterations=1, **common_params))\n",
    "\n",
    "# BAYES\n",
    "tuners.append(kt.BayesianOptimization(build_optimizer_model, project_name='bayes', **common_params))\n",
    "\n",
    "# GRID\n",
    "tuners.append(kt.GridSearch(build_optimizer_model, project_name='grid', **common_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "55979b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 Complete [00h 36m 00s]\n",
      "val_loss: 0.15532536804676056\n",
      "\n",
      "Best val_loss So Far: 0.15532536804676056\n",
      "Total elapsed time: 13h 23m 27s\n",
      "Finding best epoch\n",
      "Epoch 1/30\n",
      "7/7 [==============================] - 14s 2s/step - loss: 1911.9928 - accuracy: 0.2725 - val_loss: 147.0729 - val_accuracy: 0.5250\n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 787.3993 - accuracy: 0.3200 - val_loss: 274.2293 - val_accuracy: 0.2200\n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 566.2479 - accuracy: 0.3525 - val_loss: 280.5002 - val_accuracy: 0.5250\n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 13s 2s/step - loss: 726.5581 - accuracy: 0.3063 - val_loss: 295.4773 - val_accuracy: 0.1250\n",
      "\n",
      "Retraining to best epoch: 3\n",
      "Epoch 1/3\n",
      "89/89 [==============================] - 176s 2s/step - loss: 384.8629 - accuracy: 0.3375\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 167s 2s/step - loss: 63.5409 - accuracy: 0.4828\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 168s 2s/step - loss: 6.1380 - accuracy: 0.7960\n",
      "\n",
      "Evaluating model\n",
      "89/89 [==============================] - 10s 105ms/step - loss: 0.9508 - accuracy: 0.7847\n",
      "Loss: 0.9508249163627625\tAccuracy: 0.7847075462341309\n"
     ]
    }
   ],
   "source": [
    "# run all optimizers\n",
    "for seed, tuner in enumerate(tuners):\n",
    "    # set seed for a level playing field between optimisers\n",
    "#     tf.keras.utils.set_random_seed(seed)\n",
    "    # run the optimizer\n",
    "    best_result = run_tuner(timer, tuner, batch_size, max_epochs, callbacks)\n",
    "    \n",
    "#     # add best params to result dict and save current dict to file\n",
    "#     save_best_result(best_results, best_result, tuner)\n",
    "    if tuner.project_name in best_results.keys():\n",
    "        \n",
    "        best_results[tuner.project_name].update(best_result)\n",
    "#         print(results)\n",
    "    else:\n",
    "        best_results[tuner.project_name] = best_result\n",
    "    \n",
    "    loss, accuracy, best_epoch = retrain_and_evaluate(tuners[0])\n",
    "    \n",
    "    best_results[tuner.project_name]['loss'] = loss\n",
    "    best_results[tuner.project_name]['accuracy'] = accuracy\n",
    "    #convert from int64 to make serializable\n",
    "    best_results[tuner.project_name]['best_epoch'] = int(best_epoch)\n",
    "    \n",
    "     #save the current result to file\n",
    "    f = open(f\"{tuner.directory}/results.json\", \"w\")\n",
    "\n",
    "    json.dump(best_results, f, indent = 6)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    print(f'Loss: {loss}\\tAccuracy: {accuracy}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "77d28829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random': {'duration': '13:12:02',\n",
       "  'best_params': {'filters_1': 24,\n",
       "   'rate_1': 0.30000000000000004,\n",
       "   'learning_rate': 0.0001,\n",
       "   'filters_2': 32},\n",
       "  'loss': 1.2720849514007568,\n",
       "  'accuracy': 0.6578576564788818,\n",
       "  'best_epoch': 3},\n",
       " 'hyperband': {'duration': '02:28:52',\n",
       "  'best_params': {'filters_1': 32,\n",
       "   'rate_1': 0.2,\n",
       "   'learning_rate': 0.001,\n",
       "   'tuner/epochs': 2,\n",
       "   'tuner/initial_epoch': 0,\n",
       "   'tuner/bracket': 4,\n",
       "   'tuner/round': 0},\n",
       "  'loss': 0.8689003586769104,\n",
       "  'accuracy': 0.7431289553642273,\n",
       "  'best_epoch': 3},\n",
       " 'bayes': {'duration': '04:25:49',\n",
       "  'best_params': {'filters_1': 24, 'rate_1': 0.4, 'learning_rate': 0.0001},\n",
       "  'loss': 0.20684757828712463,\n",
       "  'accuracy': 0.9467934966087341,\n",
       "  'best_epoch': 12},\n",
       " 'grid': {'duration': '13:23:26',\n",
       "  'best_params': {'filters_1': 16, 'rate_1': 0.2, 'learning_rate': 0.0001},\n",
       "  'loss': 0.9508249163627625,\n",
       "  'accuracy': 0.7847075462341309,\n",
       "  'best_epoch': 3}}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde39636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuners[0].results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0736a595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-08-28'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "now = datetime.datetime.now()\n",
    "str(now)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bbb0bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_untrained_model():\n",
    "\n",
    "        inputs = keras.Input(shape=dh.shape)\n",
    "        x = keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs) \n",
    "        x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "        x = keras.layers.Dropout(rate=0.1)(x)\n",
    "        x = keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x) \n",
    "        x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "        x = keras.layers.Flatten()(x)\n",
    "        \n",
    "        outputs = keras.layers.Dense(dh.output_size, activation=\"softmax\")(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "   \n",
    "        return model\n",
    "\n",
    "# build a model and save its weights for use throughout\n",
    "model = build_untrained_model()\n",
    "model.save_weights('../weights/untrained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f0f98f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer_model(hp):\n",
    "    \n",
    "    model = build_untrained_model()\n",
    "    model.load_weights('../weights/untrained.h5')\n",
    "    \n",
    "    model.layers[0].filters = hp.Int(name=\"filters_1\", min_value=16, max_value=32, step=8)\n",
    "    model.layers[2].rate = hp.Float(name=\"rate_1\", min_value=0.2, max_value=0.5, step=0.1)\n",
    "    model.layers[3].filters = hp.Int(name=\"filters_1\", min_value=16, max_value=32, step=8)\n",
    "    \n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(\n",
    "            optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d38f22f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "24                |24                |filters_1\n",
      "0.4               |0.4               |rate_1\n",
      "0.01              |0.01              |learning_rate\n",
      "\n",
      "Epoch 1/30\n",
      "71/71 [==============================] - 235s 3s/step - loss: 1917.2975 - accuracy: 0.3762 - val_loss: 1.5073 - val_accuracy: 0.2162\n",
      "Epoch 2/30\n",
      " 4/71 [>.............................] - ETA: 4:07 - loss: 1.3196 - accuracy: 0.5410"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[251], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m common_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics, \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecutions_per_trial\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      9\u001b[0m tuner\u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[0;32m     10\u001b[0m     build_opt_model,\n\u001b[0;32m     11\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_random\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     12\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# common_params = {\n",
    "#     'objective': metrics, \n",
    "#     'executions_per_trial':1,\n",
    "#     'directory':directory,\n",
    "#     'tuner_id':'test_03',\n",
    "#     'overwrite':False,\n",
    "# }\n",
    "\n",
    "# tuner= kt.RandomSearch(\n",
    "#     build_opt_model,\n",
    "#     project_name='test_random', \n",
    "#     max_trials=4, \n",
    "# )\n",
    "\n",
    "# tuner.search(\n",
    "#             dh.X_train, dh.y_train,\n",
    "#             batch_size=batch_size,\n",
    "#             epochs=max_epochs,\n",
    "#             validation_split=0.2,\n",
    "#             callbacks=callbacks,\n",
    "#             verbose=1\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31e378",
   "metadata": {},
   "source": [
    "[cite]  \n",
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + logfactor(max_epochs) and rounding it up to the nearest integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af7c5a",
   "metadata": {},
   "source": [
    "## Redundant code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "     0:\n",
    "      {'hyperband':\n",
    "       {'r1':100,'r2':150},\n",
    "      'bayes':\n",
    "       {'r1':100,'r2':150}\n",
    "      },\n",
    "      1:\n",
    "      {'hyperband':\n",
    "       {'r1':100,'r2':150},\n",
    "      'bayes':\n",
    "       {'r1':100,'r2':150}\n",
    "      }\n",
    "     }\n",
    "    \n",
    "d[2] = {\"hyperband\":{}}\n",
    "d[2]['hyperband'] = {'r1':999,'r2':999}\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f07064",
   "metadata": {},
   "source": [
    "### Results\n",
    "Error \"BiasGrad requires tensor size <= int32 max\" with batch 256  \n",
    "https://stackoverflow.com/questions/60414562/how-to-solve-the-biasgrad-requires-tensor-size-int32-max-invalidargumenterr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622febbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
